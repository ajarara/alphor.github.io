* Data Analysis 584
* High level overview
** Variable analysis
Our task was to find a model that predicted the sale price of a home with respect to the following eight parameters:
*** 1stFlrSF
*** 2ndFlrSF
*** WoodDeckSF
*** OpenPorchSF
*** YearBuilt
*** BsmtSF
*** Neighborhood
*** Street
** Initial ideas
*** Elimination of street type from the model
Two levels of gravel and pavement: Only 6 datapoints deviated from the vast majority. So remove it from our model, there is little reason to include it.
The aim of most analysis is not to fit the formula to the data you have, but instead have the model accurately predict new elements. This is why we split the data into testing and training data. However, the reality is that any model we come up with that includes Street type as a parameter would be unreliable. So we drop it from the model, which lowers our degrees of freedom which makes calculation easier.
*** Certain variables were optional
X2ndFlrSF
WoodDeckSF
OpenPorchSF
BasementSF

Neighborhood: Categorical

*** Neighborhood
I wanted to place special emphasis on neighborhood, holding it as a standard candle to remove a degree of freedom, and performing exploratory analysis using the other variables. (I later show that this methodology is flawed)

** Initial exploration

*** Libraries used (and imports)
#+BEGIN_SRC R :tangle code.R

# libraries used
library(lmtest)
library(MASS)
library(nlme)
library(DAAG) # needed for cross validation

# setup
dat = read.csv("BHD.csv")
mydata = dat # some code in here refers to mydata instead of dat. They're the same thing.

#+END_SRC

*** Initial linear regression model

**** Picture
img/heteroscedastic.png
**** Desc
Go over heteroscedasticity first: since the shape is conical, this implies that there are populations within the dataset that vary differently. This provides the motivation for this section, but _conclude_ that the model would be far too difficult to characterize if we split the data in any significant matter, and would likely succumb to overfitting. When you hit the last graph, a good idea would be to mention this overfitting.

Further, there is no guarantee that neighborhood is the right classification.


*** Neighborhood
| Neighborhood | Count | mean(SalePrice) |
|--------------+-------+-----------------|
| NAmes        |   225 |        145847.1 |
| CollgCr      |   150 |        197965.8 |
| OldTown      |   113 |        128225.3 |
| Edwards      |   100 |        128219.7 |
| Somerst      |    86 |        225379.8 |
| Gilbert      |    79 |        192854.5 |
| NridgHt      |    77 |        316270.6 |
| Sawyer       |    74 |        136793.1 |
| NWAmes       |    73 |        189050.1 |
| SawyerW      |    59 |        186555.8 |
| BrkSide      |    58 |        124834.1 |
| Crawfor      |    51 |        210624.7 |
| Mitchel      |    49 |        156270.1 |
| NoRidge      |    41 |        335295.3 |
| Timber       |    38 |        242247.4 |
| IDOTRR       |    37 |        100123.8 |
| ClearCr      |    28 |        212565.4 |
| StoneBr      |    25 |          310499 |
| SWISU        |    25 |        142591.4 |
| Blmngtn      |    17 |        194870.9 |
| MeadowV      |    17 |        98576.47 |
| BrDale       |    16 |        104493.8 |
| Veenker      |    11 |        238772.7 |
| NPkVill      |     9 |        142694.4 |
| Blueste      |     2 |          137500 |

Since we have the most data to work with in NAmes, we will use this.

A couple things we have to consider when working with a selection of data: We may be introducing bias into our dataset: For example, NAmes might be a very small neighborhood in terms of square feet, and yet it has the most establishments. So we should not look to see trends between it and other neighborhoods, but instead look within it for intuition that might guide us later on.

Unfortunately this did not work very well. This method introduced very significant complexity to our would be model. We explore NAmes, CollgCr,

Only need to show one of these, the rest are not modified beyond variable names and underlying data.

#+BEGIN_SRC R :tangle code.R 

# img/pairNAmes.png
ames = dat[dat$Neighborhood == "NAmes",]
# remove Neighborhood and Street
ames = subset(ames, select = c(-Street,-Neighborhood))
logames = ames
logames$SalePrice = log(ames$SalePrice)
pairs(ames, upper.panel = my_line, main="NAmes")
#+END_SRC
img/pairNAmes.png

#+BEGIN_SRC R :tangle code.R 

# img/pairCollgCr.png
coll = dat[dat$Neighborhood == "CollgCr",]
# remove Neighborhood and Street
coll = subset(coll, select = c(-Street,-Neighborhood))
logcoll = coll
logcoll$SalePrice = log(coll$SalePrice)
pairs(coll, upper.panel = my_line, main="CollgCr")
#+END_SRC
img/pairCollgCr.png

#+BEGIN_SRC R :tangle code.R 

# img/pairTimber.png
timb = dat[dat$Neighborhood == "Timber",]
# remove Neighborhood and Street
timb = subset(timb, select = c(-Street,-Neighborhood))
logtimb = timb
logtimb$SalePrice = log(timb$SalePrice)
pairs(timb, upper.panel = my_line, main="Timber")

#+END_SRC
img/pairTimber.png

#+BEGIN_SRC R :tangle code.R 

# line of best fit function for previous pairs graphs
my_line <- function(x,y,...){
    points(x,y,...)
    abline(a = lm(y ~ x)$coefficients[1] , b = lm(y ~ x)$coefficients[2] , ...)
}
#+END_SRC

Clear correlation between 2nd floor area and pricing, OpenPorch, BsmtSF.
But this model is way too complicated. The honeymoon ends when we hit any of the neighborhoods with fewer members. We cannot rely on neighborhood to give us reliable regressions.

*** Simple linear regression
Issue: sample size
Initially when we first recieved this project, I thought the 1460 values given to us were graceful. They turned out to be a significant hindrance in allowing us to get a model that was better fitting, as there would be a lot of outliers. Further, as mentioned before with the disregarding of Street type, we should not be overfitting to our distribution. There are pitfalls to doing so, especially with such a small sample size. Simpson's paradox example.

Then the decision to go with a simple linear regression is most likely the correct one, as you can always drop out to a glm if the data you receive is contrary to your model. However, as we cannot rely on extra data in this case, the best bet is to go with the simplest one: linear regression.
*** Complete overview of our data
#+BEGIN_SRC R :tangle code.R 

# simple overview
datNoStreet = subset(dat, select = c(-Street,-Neighborhood))
pairs(datNoStreet, upper.panel = my_line, main="Overview")
#+END_SRC
img/overview.png

There are some trends we've noticed before, 1stFloor.. etc. There is no street parameter here because we've taken a subset of the data and removed it. We have not removed the neighborhood column, but since it is not binary it is not shown here by default (it would be incomprehensible at this scale anyway).

*** Log transform of response
This removes the large amount of variance as we increase the values in the linear regression.

#+BEGIN_SRC R :tangle code.R 

# Regression overview
logResponse = lm(log(SalePrice)~. - Street, data=dat)
plot(fitted(logResponse), studres(logResponse))
#+END_SRC
img/regOverview.png

However, the residuals (errors) do not follow normal distribution. What we'd like to do is use the generalized least squares estimator to establish correlation and coefficients for our model. To do this, we apply the weight of 1/(residual_i)^2

#+BEGIN_SRC R :tangle code.R 

# Weighted model
model_2=lm(log(SalePrice)~.-Street,data=mydata)
u_hat=log(mydata$SalePrice)-fitted(model_2)
u_hat=1/(u_hat**2) 
model_3=gls(log(SalePrice)~.-Street,data=mydata,weights=varFixed(~u_hat))
qqnorm(resid(model_3))
qqline(resid(model_3))
#+END_SRC
img/weightedModel.png

Since the errors were normally distributed, we have confidence in these results with the specific weight.

The resulting summary showed us which variables significantly affected housing price in Boston.

Among these is the 1st floor, second floor, wood deck square footage, and basement, year built. The coefficients for these are small, but that is because they are per unit increase (ie 1 square foot of a second floor would increase the log(SalePrice) by a small constant, and subsequently the SalePrice by a percentage), hence the discrepancy between the coefficient size and significance.

Following this, we tried various models, some of them linear, but found the best success with segmenting the data based on optional variables, specifically the 2nd floor, allowing us to see the (somewhat mild) interaction between the 1st and second floor SF in the prediction of our salesprice.

Intermediary models
#+BEGIN_SRC R :tangle code.R 

# Intermediary models
model_4=lm(log(SalePrice)~(X1stFlrSF+X2ndFlrSF+
             WoodDeckSF+OpenPorchSF+BsmtSF+Neighborhood+YearBuilt)**2+
             I(X1stFlrSF**2)+I(X2ndFlrSF**2)+
             I(WoodDeckSF**2)+I(BsmtSF**2),data=mydata)
             
#we can see that I(X1stFlrSF^2),X1stFlrSF:X2ndFlrSF is significant,           
summary(model_4)
qqnorm(resid(model_4))
qqline(resid(model_4))


data_1=mydata[mydata$X2ndFlrSF > 0,]
#=data_1[data_1$WoodDeckSF > 0,]
model_51=lm(log(SalePrice)~.+I(X1stFlrSF**2)+X1stFlrSF:X2ndFlrSF-Street,
            data=data_1)
summary(model_51)
model_52=stepAIC(model_51,direction="both")
#we can get the following model which is best fit
#log(SalePrice) ~ X1stFlrSF + X2ndFlrSF + WoodDeckSF + OpenPorchSF + 
 # Neighborhood + BsmtSF + YearBuilt + I(X1stFlrSF^2)
data_2=mydata[mydata$X2ndFlrSF == 0,]
model_61=lm(log(SalePrice)~.+I(X1stFlrSF**2)-Street,
            data=data_2)
summary(model_61)
model_62=stepAIC(model_61,direction="both")

#+END_SRC

Then came the stepwise analysis, however, with the stepwise AIC (conducted in both directions for multiple models detailed in the assoc'd R report), only the interaction random variable between 1st and 2nd floor SF was added, only in the case of the second floor being included. So we added the interaction to our model, producing:


| Parameter           |  Coefficients |
|---------------------+---------------|
| X1stFlrSF           |  0.0008023042 |
| X2ndFlrSF           |  0.0005162776 |
| WoodDeckSF          |   0.000210046 |
| OpenPorchSF         |  0.0003655512 |
| BsmtSF              |  0.0001368659 |
| YearBuilt           |   0.004020196 |
| X1stFlrSF^2         | -1.173194e-07 |
| X1stFlrSF:X2ndFlrSF | -1.504619e-07 |
(neighborhood omitted for brevity)

*** Data splitting was done binomially
We created a vector of the length of the dataset, with p = 0.75. Those with observation == 1 were in our training sample, and those without were not. This did have a tendency to eliminate some df specifically with regard to the Neighborhood field. To work around this (by instead selecting proportionally from each neighborhood) would introduce significant selection bias, but there is definite value to keeping neighborhoods in the model, so we did.


*** Predictions

#+BEGIN_SRC R :tangle code.R 

# Predictions
selection=rbinom(1460, 1, 0.75)
data_training = mydata[selection == 1,]
data_testing = mydata[selection == 0,]

model_accepted <- lm(log(SalePrice) ~ X1stFlrSF + X2ndFlrSF + WoodDeckSF + OpenPorchSF + Neighborhood + BsmtSF + YearBuilt + I(X1stFlrSF^2) + X1stFlrSF:X2ndFlrSF,data=data_training)

pre_testing = predict(model_accepted, new=data.frame(data_testing))
plot(pre_testing, log(data_testing$SalePrice))

#+END_SRC
img/residTesting.png

Pretty good!

*** Cross validation?
#+BEGIN_SRC R :tangle code.R 

# Cross validation
CVlm(dat,model_final, m=10)
#+END_SRC
img/crossValidation.png

A more strenuous test, it essentially replicates the previous testing, removing 1/10th of the data each time, obtaining the linear coefficients, and giving us the predictions of the removed data with the coefficients generatied from the training data.
The sum of squares was 4.23

*** Additional musings
**** Training data proportionally selected from neighborhood
Initially I wanted to see if I could get a good model from proportionally selecting data by neighborhood. However, there was no way I would guarantee a representative of each neighborhood in both the testing and training data without artificially selecting them by hand, which is a mistake. So I resisted the temptation.

**** Outliers
We did not attempt to treat the outliers. In file img/outincrvl.png, there are a couple of outliers that have very significant leverage, one that is way off the line.

show img/outlierboxplot.png

We remove the outliers above the IQR by sorting and iterating through a for loop, removing the top 10 values (there are no outliers in the boxplot below the IQR).

| Parameter           |  Coefficients | New Coefficients | Percentage |
|---------------------+---------------+------------------+------------|
| X1stFlrSF           |  0.0008023042 |         0.000972 |  21.151055 |
| X2ndFlrSF           |  0.0005162776 |         0.000545 |  5.5633636 |
| WoodDeckSF          |   0.000210046 |         0.000216 |  2.8346172 |
| OpenPorchSF         |  0.0003655512 |         0.000265 | -27.506735 |
| BsmtSF              |  0.0001368659 |         0.000138 | 0.82862130 |
| YearBuilt           |   0.004020196 |          0.00361 | -10.203383 |
| X1stFlrSF^2         | -1.173194e-07 |        -1.74e-07 |  48.313067 |
| X1stFlrSF:X2ndFlrSF | -1.504619e-07 |        -1.58e-07 |  5.0099726 |
#+TBLFM: $4=100 * ($3-$2)/$2
