* Data Analysis 584
* High level overview
** Variable analysis
Our task was to find a model that predicted the sale price of a home with respect to the following eight parameters:
*** 1stFlrSF
*** 2ndFlrSF
*** WoodDeckSF
*** OpenPorchSF
*** YearBuilt
*** BsmtSF
*** Neighborhood
*** Street
** Initial ideas
*** Elimination of street type from the model
Two levels of gravel and pavement: Only 6 datapoints deviated from the vast majority. So remove it from our model, there is little reason to include it.
The aim of most analysis is not to fit the formula to the data you have, but instead have the model accurately predict new elements. This is why we split the data into testing and training data. However, the reality is that any model we come up with that includes Street type as a parameter would be unreliable. So we drop it from the model, which lowers our degrees of freedom which makes calculation easier.
*** Certain variables were optional
X2ndFlrSF
WoodDeckSF
OpenPorchSF
BasementSF

Neighborhood: Categorical

*** Neighborhood
I wanted to place special emphasis on neighborhood, holding it as a standard candle to remove a degree of freedom, and performing exploratory analysis using the other variables. (I later show that this methodology is flawed)
** Me
*** Neighborhood
| Neighborhood | Count | mean(SalePrice) |
|--------------+-------+-----------------|
| NAmes        |   225 |        145847.1 |
| CollgCr      |   150 |        197965.8 |
| OldTown      |   113 |        128225.3 |
| Edwards      |   100 |        128219.7 |
| Somerst      |    86 |        225379.8 |
| Gilbert      |    79 |        192854.5 |
| NridgHt      |    77 |        316270.6 |
| Sawyer       |    74 |        136793.1 |
| NWAmes       |    73 |        189050.1 |
| SawyerW      |    59 |        186555.8 |
| BrkSide      |    58 |        124834.1 |
| Crawfor      |    51 |        210624.7 |
| Mitchel      |    49 |        156270.1 |
| NoRidge      |    41 |        335295.3 |
| Timber       |    38 |        242247.4 |
| IDOTRR       |    37 |        100123.8 |
| ClearCr      |    28 |        212565.4 |
| StoneBr      |    25 |          310499 |
| SWISU        |    25 |        142591.4 |
| Blmngtn      |    17 |        194870.9 |
| MeadowV      |    17 |        98576.47 |
| BrDale       |    16 |        104493.8 |
| Veenker      |    11 |        238772.7 |
| NPkVill      |     9 |        142694.4 |
| Blueste      |     2 |          137500 |

Since we have the most data to work with in NAmes, we will use this.

A couple things we have to consider when working with a selection of data: We may be introducing bias into our dataset: For example, NAmes might be a very small neighborhood in terms of square feet, and yet it has the most establishments. So we should not look to see trends between it and other neighborhoods, but instead look within it for intuition that might guide us later on.
**** Initial overview
IMPORTANT: Go over heteroscedasticity first: since the shape is conical, this implies that there are populations within the dataset that vary differently. This provides the motivation for this section, but _conclude_ that the model would be far too difficult to characterize if we split the data in any significant matter, and would likely succumb to overfitting. When you hit the last graph, a good idea would be to mention this overfitting.

Further, there is no guarantee that neighborhood is the right classification.

Unfortunately this did not work very well. This method introduced very significant complexity to our would be model. 

#+BEGIN_SRC 
ames = dat[dat$Neighborhood == "NAmes",]
# remove Neighborhood and Street
ames = subset(ames, select = c(-Street,-Neighborhood))
logames = ames
logames$SalePrice = log(ames$SalePrice)
pairs(ames, upper.panel = my_line, main="NAmes")
#+END_SRC

#+BEGIN_SRC 
coll = dat[dat$Neighborhood == "CollgCr",]
# remove Neighborhood and Street
coll = subset(coll, select = c(-Street,-Neighborhood))
logcoll = coll
logcoll$SalePrice = log(coll$SalePrice)
pairs(coll, upper.panel = my_line, main="CollgCr")
#+END_SRC

#+BEGIN_SRC 
coll = dat[dat$Neighborhood == "CollgCr",]
# remove Neighborhood and Street
coll = subset(coll, select = c(-Street,-Neighborhood))
logcoll = coll
logcoll$SalePrice = log(coll$SalePrice)
pairs(coll, upper.panel = my_line, main="CollgCr")
#+END_SRC

#+BEGIN_SRC 

timb = dat[dat$Neighborhood == "Timber",]
# remove Neighborhood and Street
timb = subset(timb, select = c(-Street,-Neighborhood))
logtimb = timb
logtimb$SalePrice = log(timb$SalePrice)
pairs(timb, upper.panel = my_line, main="Timber")

#+END_SRC


#+BEGIN_SRC 
my_line <- function(x,y,...){
    points(x,y,...)
    abline(a = lm(y ~ x)$coefficients[1] , b = lm(y ~ x)$coefficients[2] , ...)
}

#+END_SRC

Clear correlation between 2nd floor area and pricing, OpenPorch, BsmtSF.
** Liping

There is clearly skew. (pg 4 shows a good way to visualize this)
To remove skew, try log transform on salePrice
Split the data in two, with rbinom (could be improved)

Slices based on optional parameters.

slice1_1 is the slice with no deck or porch
slice1_2 is no deck, yes porch
slice2_1 is yes deck, no porch
slice2_2 is yes deck, yes porch

Waiting on liping for analysis of these slices.
** Zhezhi:
*** initial model was a simple regression against all parameters
(including street pavement)
Found that there was heteroskedasticity in the fitted compared to the residuals of the response variable. We need to close off the cone in order to come up with a reasonably accurate model.

Show pic1: this is the model with street type included, salesprice as a response
#+BEGIN_SRC 
lmStreet = lm(SalePrice~., data=dat)
plot(fitted(lmStreet), studres(lmStreet)
#+END_SRC
Show pic2: this is the model with it not included.

#+BEGIN_SRC 
lmNoStreet = lm(SalePrice~. - Street, data=dat)
plot(fitted(lmNoStreet), studres(lmNoStreet))
#+END_SRC

There is little difference. There is a possibility that streets provide some insight on other variables in the distribution, ie it correlated with how new a house is (it does not, all but one were built before 1965), but any conclusions based on this would likely not hold up when exposed to a bigger dataset.

Removing Neighborhoods from the model significantly changed the shape of the distribution. Then we kept it in.
*** Log transform of response
This removes the large amount of variance as we increase the values in the linear regression.

#+BEGIN_SRC 
logResponse = lm(log(SalePrice)~. - Street, data=dat)
plot(fitted(logResponse), studres(logResponse))
#+END_SRC

However, the residuals (errors) do not follow normal distribution. What we'd like to do is use the generalized least squares estimator to establish correlation and coefficients for our model. To do this, we apply the weight of 1/(residual_i)^2

#+BEGIN_SRC 
model_2=lm(log(SalePrice)~.-Street,data=mydata)
u_hat=log(mydata$SalePrice)-fitted(model_2)
u_hat=1/(u_hat**2) 
model_3=gls(log(SalePrice)~.-Street,data=mydata,weights=varFixed(~u_hat))
qqnorm(resid(model_3))
qqline(resid(model_3))
#+END_SRC

The resulting summary showed us which variables significantly affected housing price in Boston.

Among these is the 1st floor, second floor, wood deck square footage, and basement, year built. The coefficients for these are small, but that is because they are per unit increase (ie 1 square foot of a second floor would increase the log(SalePrice) by a small constant, and subsequently the SalePrice by a percentage), hence the discrepancy between the coefficient size and significance.

Following this, we tried various models, some of them linear, but found the best success with segmenting the data based on optional variables, specifically the 2nd floor:

Then came the stepwise analysis, however, with the stepwise AIC (conducted in both directions for multiple models detailed in the assoc'd R report), only the interaction random variable between 1st and 2nd floor SF was added, only in the case of the second floor being included. So we added the interaction to our model, producing:

| Parameter           |  Coefficients |
|---------------------+---------------|
| X1stFlrSF           |  0.0008023042 |
| X2ndFlrSF           |  0.0005162776 |
| WoodDeckSF          |   0.000210046 |
| OpenPorchSF         |  0.0003655512 |
| BsmtSF              |  0.0001368659 |
| YearBuilt           |   0.004020196 |
| X1stFlrSF^2         | -1.173194e-07 |
| X1stFlrSF:X2ndFlrSF | -1.504619e-07 |
(neighborhood omitted for brevity)
  
model_exm<-lm(SalePrice ~ X1stFlrSF + X2ndFlrSF + WoodDeckSF + OpenPorchSF + Neighborhood + BsmtSF + YearBuilt + I(X1stFlrSF^2) + X1stFlrSF:X2ndFlrSF,data=data_true)

*** Data splitting was done binomially
We created a vector of the length of the dataset, with p = 0.75. Those with observation == 1 were in our training sample, and those without were not. This did have a tendency to eliminate some df specifically with regard to the Neighborhood field. To work around this (by instead selecting proportionally from each neighborhood) would introduce significant selection bias, but there is definite value to keeping neighborhoods in the model, so we did.

*** Predictions
