#+SETUPFILE: theme-readtheorg.setup
* Introduction
What follows is a small compilation of just some exploratory analysis I've done. 

Let me know if you'd like anything to be published, right now all I'm doing is separating out the data. One thing I'd like to make note of eventually is the correlation between features of the house. It's clear that having a porch or a 2nd floor effects the price positively, but I'm sure there is some correlation between the two. 

There probably exists some degree of correlation between all the features of the house. It's just a matter of what are the ones that affect pricing the most. I don't really know how to structure the analysis, something I'd welcome feedback on. As for now, I'm just going to get basic info.

* Questions:
In what ways does the data affect pricing?
Are there any correlations between the variables themselves?
Distributions of each variable irrespective of their affiliations
Outliers?
** Slide format

So LaTeX has a presentation mode, which the professor uses. There are a lot of themes, but the default theme is probably the best looking.

Using the Beamer class, pittsburg theme or bust (aka default theme)
** Some tooling for the data
*** Presumptions
All the code blocks after this section assume the following:
#+BEGIN_SRC
dat = read.csv("BHD.csv")
#+END_SRC

I have dplyr installed to allow for SQL-like manipulations and selections, so I'll be using it.
#+BEGIN_SRC 
install.package("dplyr")
#+END_SRC

(Installing packages usually takes a while)

*** Some useful language constructs
Data frame indexing. Let's say you want to get only the apartments in Mitchel. It's quite simple to get this:
#+BEGIN_SRC
dat[dat$Neighborhood == "Mitchel",]
#+END_SRC

The last comma is necessary, it is used to get the columns. In fact, you can use this to immediately get the data for a specific neighborhood, but this is a little tedious as you have to refer to the column names by numbers (ie SalePrice is column 1, WoodDeckSF is column 4, etc.)

#+BEGIN_SRC
mean(dat[dat$Neighborhood == "Mitchel", 1]) # 156,270
#+END_SRC

Multi parameter data frame indexing:

#+BEGIN_SRC 
# get OpenPorchSF between quartiles
quart = c(0, 0.25, 0.5, 0.75, 1)
results = quantile(dat$OpenPorchSF, probs = quart)

low = dat[dat$OpenPorchSF <= results[[1]],]
mediocre = dat[dat$OpenPorchSF <= results[[2]],]
good = dat[dat$OpenPorchSF <= results[[3]],]
great = dat[dat$OpenPorchSF <= results[[4]],]

#+END_SRC

#+BEGIN_SRC 
# get WoodDeckSF between quartiles
quart = c(0, 0.25, 0.5, 0.75, 1)
results = quantile(dat$WoodDeckSF, probs = quart)

low = dat[dat$WoodDeckSF <= results[[1]],]
mediocre = dat[dat$WoodDeckSF <= results[[2]],]
good = dat[dat$WoodDeckSF <= results[[3]],]
great = dat[dat$WoodDeckSF <= results[[4]],]


#+END_SRC
There are ways to perform analyses on slices of the data. For example, let's say I want to only analyze trends in a specific neighborhood. You can manually slice these each time, but that is very time intensive. Here is a function that takes a dataset (in this case, our BHD dat table), a label (like SalePrice, OpenPorchSF), and shows you the Cullen-Frey graph of the distribution of the attribute (it only works on integers).

#+BEGIN_SRC 
library(fitdistrplus) # we need descdist() from this package

descatt <- function (dataset, attribute) {
    descdist(getElement(dataset, attribute))
}

# Usage

descatt(dat, "SalePrice")

# summary statistics
# ------
# min:  34900   max:  755000 
# median:  163000 
# mean:  180921.2 
# estimated sd:  79442.5 
# estimated skewness:  1.882876 
# estimated kurtosis:  9.536282 

#+END_SRC

It is trivial to extend this. Here is an example I use to generate the below charts quickly:

#+BEGIN_SRC R
  saveDescAttToImg <- function (dataset, attribute, imgName=NULL) {
      if(is.null(imgName)) {
          imgName = paste("img/descdist", attribute, ".png" sep="")
      }
      png(imgName)
      descatt(dataset, attribute)
      dev.off()
  }
#+END_SRC
*** Functions
Try as I might, I could not find a built-in function that separated data by an arbitrary constraint. So I'll resort to using dplyr's filter.

Categorical variable mapping (mostly for neighborhood analysis)
#+BEGIN_SRC R
   catvarmap <- function(dataset, filter, attribute, somefunc) {
       for(level in levels(getElement(dataset, filter))) {
           print(
               cat(level, 
                   somefunc(
                       getElement(
                           filter(dataset, getElement(dataset, filter) == level),
                           attribute)
                   ),
                   "\n", sep = "\t")
           )
       }
   }
#+END_SRC


** Rooms
SalePrice is definitely affected by presence of specific features of the house. Not sure whether the influence is categorical.

*** Basement

**** Categorical data
#+BEGIN_SRC 
hasbasement = dat[dat$BsmtSF != 0,] # has a basement 
nobasement = dat[dat$BsmtSF == 0,] # doesn't have a basement

nrow(dat) == nrow(hasbasement) + nrow(nobasement) # TRUE

mean(hasbasement$SalePrice) - mean(nobasement$SalePrice) # 22,645

#+END_SRC

**** Distribution parameters
Uh... the cullen and Frey graph does not show anything that is coherent. Possibilities include trimming the exteriors.
#+BEGIN_SRC 
hasbasement = dat[dat$BsmtSF != 0,] # has a basement

descdist(hasbasement$BsmtSF)

# summary statistics
# ------
# min:  2   max:  5644 
# median:  604 
# mean:  652.28 
# estimated sd:  411.9803 
# estimated skewness:  2.302275 
# estimated kurtosis:  24.32368 

#+END_SRC

[[./img/descdistBasementSF.png][The descdist produces this image]].
**** Categorical data
#+BEGIN_SRC 
hasDeck = dat[dat$WoodDeckSF != 0,] # has a wood deck
noDeck = dat[dat$WoodDeckSF == 0,] # doesn't have a wood deck

nrow(dat) == nrow(hasDeck) + nrow(noDeck) # TRUE

mean(hasDeck$SalePrice) - mean(noDeck$SalePrice) # 47,320

#+END_SRC

**** Distribution parameters


#+BEGIN_SRC 
hasDeck = dat[dat$WoodDeckSF != 0,] # has a wood deck

descdist(hasDeck$WoodDeckSF)

# summary statistics
# ------
# min:  12   max:  857 
# median:  171 
# mean:  196.8484 
# estimated sd:  112.2997 
# estimated skewness:  1.617617 
# estimated kurtosis:  7.286244 

#+END_SRC

[[./img/descdistWoodDeckSF.png][The descdist produces this image]]. This looks a lot more promising than the previous image, which does not resemble anything comprehensible.
*** 2nd Floor
**** Categorical data
#+BEGIN_SRC 
has2ndFlr = dat[dat$X2ndFlrSF != 0,] # has a 2nd floor
no2ndFlr = dat[dat$X2ndFlrSF == 0,] # doesn't have a 2nd floor

nrow(dat) == nrow(has2ndFlr) + nrow(no2ndFlr) # TRUE

mean(has2ndFlr$SalePrice) - mean(no2ndFlr$SalePrice) # 22,067

#+END_SRC

*** Open Porch
**** Categorical data
#+BEGIN_SRC 
hasOpenPorch = dat[dat$OpenPorchSF != 0,] # has an open porch
noOpenPorch = dat[dat$OpenPorchSF == 0,] # doesn't have an open porch

nrow(dat) == nrow(hasOpenPorch) + nrow(noOpenPorch) # TRUE

mean(hasOpenPorch$SalePrice) - mean(noOpenPorch$SalePrice) # 65930

#+END_SRC

**** Distribution Parameters
It matches the CDF reasonably well, but the upper half does not follow the QQ plot at all. Possibly bimodal distribution?


#+BEGIN_SRC 
hasOpenPorch = dat[dat$OpenPorchSF != 0,] # has an open porch

descdist(dat$OpenPorchSF)

# summary statistics
# ------
# min:  4   max:  547 
# median:  63 
# mean:  84.73134 
# estimated sd:  68.89317 
# estimated skewness:  2.24855 
# estimated kurtosis:  10.80962 

fit.gamma

# Fitting of the distribution ' gamma ' by maximum likelihood 
# Parameters:
#         estimate  Std. Error
# shape 2.02408873 0.093442109
# rate  0.02389128 0.001248555


#+END_SRC

*** Street Access
**** Categorical data
There are only 6 houses in the entire set that have gravel driveways. All but one were constructed before 1965.
#+BEGIN_SRC 
pavedRoad = dat[dat$Street == "Pave",] # has a pavement driveway
gravelRoad = dat[dat$Street == "Grvl",] # has a gravel driveway

nrow(dat) == nrow(pavedRoad) + nrow(gravelRoad) # TRUE

mean(pavedRoad$SalePrice) - mean(gravelRoad$SalePrice) # 50940

#+END_SRC

* Analysis

** Incredibly low sample size -> bootstrapping
A good idea to mitigate this is to bootstrap. I didn't do it in the below analysis, but there is a library for it (conveniently labelled boot). Usage is simple:
#+BEGIN_SRC 
# define a statistic you'd like to test. It will get passed a formula, the dataset, and indices (generated from the bootstrap)
ressq <- function (formula, data, indices) {
    newdat = data[indices,]
    fit = lm(formula, data=newdat)
    return(summary(fit)$r.square)
}

# bootstrapping with 1000 replications
res = boot(data=dat, statistic=ressq, R=1000, formula=SalePrice~OpenPorchSF+X1stFlrSF)

# the results look suspiciously like the normal distribution (because of the CLT?)


#+END_SRC

Note: boot does not give access to the generated samples (the memory requirements would be insane if it saved each trial). It's essentially a glorified map.

Code inspired from: http://www.statmethods.net/advstats/bootstrapping.html
** Distribution of neighborhoods

#+BEGIN_SRC
as.data.frame(table(dat$Neighborhood)) # gets count of neighborhoods

catvarmap(dat, "Neighborhood", "SalePrice", mean) # gets avg price of each neighborhood.

#+END_SRC

yields (formatted for easier reading):
| Neighborhood | Count | mean(SalePrice) |
|--------------+-------+-----------------|
| NAmes        |   225 |        145847.1 |
| CollgCr      |   150 |        197965.8 |
| OldTown      |   113 |        128225.3 |
| Edwards      |   100 |        128219.7 |
| Somerst      |    86 |        225379.8 |
| Gilbert      |    79 |        192854.5 |
| NridgHt      |    77 |        316270.6 |
| Sawyer       |    74 |        136793.1 |
| NWAmes       |    73 |        189050.1 |
| SawyerW      |    59 |        186555.8 |
| BrkSide      |    58 |        124834.1 |
| Crawfor      |    51 |        210624.7 |
| Mitchel      |    49 |        156270.1 |
| NoRidge      |    41 |        335295.3 |
| Timber       |    38 |        242247.4 |
| IDOTRR       |    37 |        100123.8 |
| ClearCr      |    28 |        212565.4 |
| StoneBr      |    25 |          310499 |
| SWISU        |    25 |        142591.4 |
| Blmngtn      |    17 |        194870.9 |
| MeadowV      |    17 |        98576.47 |
| BrDale       |    16 |        104493.8 |
| Veenker      |    11 |        238772.7 |
| NPkVill      |     9 |        142694.4 |
| Blueste      |     2 |          137500 |

#+BEGIN_SRC R
quantile(as.data.frame(table(dat$Neighborhood))$Freq, probs=c(0.25, 0.5, 0.75))
#+END_SRC
| 25% | 50% | 75% |
|-----+-----+-----|
|  25 |  49 |  77 |

For the rest of the analysis, we will sample from NAmes, NrigHt, Mitchel, and StoneBr, and the dataset as a whole. My reasoning, as explained before, is that Neighborhood is the easiest parameter to control for (as it is categorical, and historically a strong invariant for pricing). This may help us look past its effects.

I choose the four neighborhoods listed out here because it is quite tedious to analyze aspects from each individual one (plus the amount of data generated would be too much to analyze meaningfully). So I chose from the upper four quantiles, and will analyze based on these.

It has been mentioned that we sample from our dataset for training and for testing. Since we have such a limited sample size (which is clearly a problem, as shown by the table above), I took the liberty of using 75% of the data to establish a training set, and the other 25% as testing. In order to provide consistent results, I've done the following:
*** Generated random indices for each of the subsets
The point is to have training data consistent across sessions between all of us.

This is simple, all I did was sample 0.75 * $index from 1:$index of each partition mentioned above, without replacement (replace=F), and paste them here. This is available verbatim as [[file:init.R][an executable R file]], but I inline it here because space on the internet is free!

#+BEGIN_SRC R :tangle init.R
    # full dataset consists of 1460 entries
    # sample(1:1460, 1460, replace=F)
    fulldatIndices = c(221,1001,419,1368,580,154,877,233,1367,266,1263,152,1122,122,831,205,685,1242,76,358,731,518,629,962,874,440,732,234,555,142,474,1062,444,1423,302,155,567,319,662,173,695,1439,180,1066,708,177,1169,1304,1252,220,1360,1161,1436,197,282,1065,1325,366,1415,745,571,13,804,1390,1320,393,722,239,505,457,546,1432,913,828,756,1458,718,1377,1201,175,498,1225,1184,162,374,703,549,1336,647,777,891,1249,314,761,445,378,1097,965,81,448,468,124,1070,617,349,1123,1355,1040,351,272,1300,861,390,304,1234,2,859,737,487,631,1158,1226,417,561,20,663,692,55,494,1133,149,824,1291,195,1438,928,587,1371,1276,344,190,993,740,217,1297,557,742,961,362,278,1052,1273,391,959,1443,80,531,163,296,226,1,301,1008,1386,660,876,1038,248,818,1050,109,661,335,150,442,610,1205,927,543,1089,988,757,53,279,1447,996,657,499,1009,24,405,295,1092,579,47,902,977,287,1452,970,1280,451,1238,597,414,944,1172,1382,969,843,1022,698,164,840,290,899,1080,836,837,1331,397,872,485,428,168,403,360,1053,1114,509,131,102,19,179,1128,10,297,285,550,1139,267,395,426,882,311,916,1071,789,577,766,729,776,1335,614,1146,1244,554,769,247,537,644,253,622,1264,182,528,862,504,1170,848,392,104,1243,90,355,1060,1449,185,1253,1192,1177,852,416,540,201,1079,565,978,939,342,821,472,450,324,787,1309,410,633,243,1034,370,858,1366,1119,1215,975,1339,601,425,1190,733,954,471,1250,1173,667,495,373,348,357,265,204,736,138,724,1075,1375,1278,790,224,94,367,1068,851,1235,1414,1419,576,838,914,1303,251,1171,484,839,749,434,345,219,320,1256,111,935,359,339,1376,1269,544,1024,688,229,802,1191,1231,1446,4,77,241,870,464,1255,1056,169,595,793,551,1106,1396,514,379,734,369,696,1389,1268,1384,569,1224,623,68,1435,167,883,1147,1362,1202,1310,1063,1287,1398,207,864,386,306,743,1045,135,589,624,1032,1241,602,974,1308,627,931,1392,1430,1117,1240,230,1103,73,966,66,1026,72,400,423,1196,262,873,383,1059,346,548,57,159,933,1457,538,1198,329,605,697,435,1064,87,1074,910,1091,1150,404,700,1456,829,64,83,470,1454,1016,456,666,1153,292,333,223,513,1143,1330,1422,1317,814,326,108,529,808,779,628,886,1296,1058,680,1294,106,682,1014,261,613,431,315,365,951,502,430,516,350,18,203,143,1341,506,986,1418,465,760,1151,547,813,62,52,313,526,1220,679,96,240,592,82,717,447,1193,305,798,1448,578,286,1204,812,879,51,819,1433,1316,1218,758,1283,1076,242,833,30,340,1111,126,815,421,275,984,136,308,1112,116,1167,690,259,176,683,1087,214,1378,907,983,994,45,330,553,919,1337,1351,1078,478,1099,594,1275,739,1453,1212,783,817,1343,900,1100,911,849,354,482,385,1129,825,1324,1221,67,953,99,93,107,947,510,9,63,832,895,768,1426,1421,989,1233,943,704,945,1350,1156,963,341,1245,35,380,32,174,237,1261,654,309,1444,1197,1003,34,932,1401,937,946,1210,1424,1000,1301,493,1149,1285,904,114,727,1229,860,905,934,1098,263,1440,620,1134,884,938,299,971,388,973,748,590,235,1437,225,635,59,684,134,422,572,1104,429,686,112,466,1413,1281,805,967,276,721,1399,1155,591,1096,270,765,607,1237,250,1344,198,694,1329,1340,1131,521,213,490,481,1200,612,1148,775,990,145,343,211,1082,1188,1054,1232,878,258,293,338,788,277,508,799,949,1175,28,1037,909,1262,782,133,678,361,364,1460,249,11,1404,728,645,192,127,58,936,31,202,1230,715,593,271,673,424,792,294,86,245,640,869,822,582,1145,574,1127,480,898,1203,1356,139,110,751,1388,1429,1284,1025,558,854,773,1163,800,889,394,455,1214,950,527,1174,337,1288,1017,650,401,1346,1004,228,352,130,875,674,871,1292,1019,125,1219,1322,1290,1282,1006,1348,1028,634,289,517,1374,980,637,1227,268,890,37,791,1216,16,377,866,500,1013,8,274,38,356,801,1168,1126,1012,281,999,867,519,387,1272,1357,598,844,795,492,1387,559,332,1105,1030,307,1031,1323,1239,1266,997,216,892,74,71,1061,632,664,1311,1027,929,1095,1055,512,54,524,1110,868,926,1407,1411,545,714,1157,771,930,170,1142,193,520,1393,539,906,1069,1279,1260,1093,46,1318,1023,1299,699,222,894,611,1361,920,254,300,146,725,128,942,1332,1302,409,596,1162,439,200,454,60,199,952,1051,1408,1165,1195,998,322,1352,1209,132,609,855,689,1199,396,312,98,564,1185,6,1359,652,1186,1286,1416,1083,573,741,26,1044,1379,325,323,328,1036,105,232,880,1410,972,255,1207,670,857,581,371,158,1029,762,92,1113,719,1381,1081,88,1189,992,496,462,1455,691,372,12,1208,260,1002,968,541,655,955,1154,618,1166,184,1397,1412,639,841,1039,402,827,1107,676,1342,1015,705,515,100,165,21,331,511,1417,1254,48,438,912,244,1005,1365,412,811,1431,91,1345,238,79,1067,1041,17,583,588,1257,1380,256,1088,1289,1270,1020,413,157,1338,1118,957,1327,1121,408,716,706,1217,638,218,280,1152,881,461,915,987,1086,288,171,523,759,1178,532,1307,1010,803,625,616,209,363,707,399,441,735,1405,1347,608,236,269,888,347,389,1251,1395,1328,1428,1033,1183,14,160,669,151,1383,1247,1130,982,575,44,264,747,630,820,15,1427,336,1434,772,649,115,672,1057,560,433,420,49,183,172,653,284,738,181,1125,806,918,137,778,568,636,368,50,1136,56,411,1101,1181,659,522,208,646,231,542,334,797,7,693,1213,1182,459,1293,118,1394,1085,1035,1180,604,1048,1277,166,1354,317,458,418,1385,850,807,273,1358,1137,599,816,671,89,140,1141,1400,641,1259,784,1406,1094,491,1164,479,318,1211,103,621,940,113,283,1403,188,353,398,148,903,452,85,1109,449,1108,415,501,755,376,227,1265,407,41,1298,1258,941,291,921,1313,33,43,473,310,713,194,981,1187,42,964,1160,36,427,443,770,823,856,483,382,469,897,810,1222,406,763,584,826,923,847,901,619,1115,1140,186,948,1442,384,1084,121,1306,120,1445,1011,925,147,1369,1315,153,381,161,1116,726,533,668,642,3,846,1334,1441,566,917,750,780,1228,1271,552,1047,896,95,1373,1132,1420,1314,908,1138,893,752,774,178,585,712,702,101,711,22,534,84,710,991,1353,536,835,432,436,1049,863,958,69,754,865,327,65,129,730,525,196,489,1391,1206,1450,887,486,853,1274,252,1370,476,556,257,1124,119,475,796,648,141,497,600,1073,656,189,1321,191,794,1072,446,453,1333,658,27,40,1305,1248,1459,615,979,746,1349,477,767,785,1364,834,507,1135,845,677,1046,723,75,375,1326,1042,1077,1223,488,1246,687,530,1295,1319,144,830,70,212,781,23,535,643,922,606,651,1120,1451,117,1312,298,1144,1159,995,1043,210,1007,1363,603,61,1372,123,460,786,206,764,956,924,720,321,1090,665,97,39,78,303,675,809,1194,1179,187,467,1402,709,437,1236,156,885,842,1425,5,562,316,29,753,985,1021,1018,215,570,586,701,1102,960,503,1176,25,246,1409,626,463,1267,563,976,744,681)


  # now trainingdatIndices are the first 1460*0.75 = 1095 of fulldatIndices.
  trainingdat = dat[fulldatIndices[1:1095],]

  # and testingdatIndices are the others!
  testingdat= dat[fulldatIndices[1096:1460],]


 #+END_SRC

This way we ensure that there is no collision between these two.
BUT! The problem arrives when we try to do this same method for our partitioned data. There is bound to be some mixing of training data that is testing data in the full sample. I don't quite know how to solve this problem, or frankly, if it's a problem at all (as we are training in diffferent contexts).

Anyway, here is the rest of the indices for the four partitions:
#+BEGIN_SRC R :tangle init.R
  # fullNAmesIndices = sample(1:225, 225, replace=F) # 75% 169
  # fullNrigHtIndices = sample(1:77, 77, replace=F) # 75% 58
  # fullMitchelIndices = sample(1:49, 49, replace=F) # 75% 35
  # fullStoneBrIndices = sample(1:25, 25, replace=F) # 75% 18 ish

  fullNAmesIndices =
  c(213,23,175,179,28,96,130,95,113,104,94,156,141,76,212,116,54,18,127,14,194,149,51,126,159,15,106,136,52,176,87,66,25,101,88,89,62,65,162,143,58,209,99,206,98,78,82,146,160,172,31,84,38,85,45,208,21,102,16,198,97,167,77,133,191,60,171,154,2,100,32,33,4,109,114,35,112,39,165,161,148,29,152,188,7,142,108,197,69,36,20,105,202,203,118,223,46,79,174,47,220,168,42,3,200,205,177,72,8,83,187,224,40,219,41,196,173,199,155,107,86,71,145,110,210,129,182,103,211,189,68,147,134,183,140,22,222,59,44,144,158,80,57,53,11,56,13,153,81,186,120,216,185,192,169,67,55,164,163,217,201,180,91,193,74,139,195,128,151,157,137,166,6,190,93,43,92,34,138,215,50,10,37,17,64,170,30,26,111,73,204,63,90,178,135,132,225,27,115,75,9,214,24,122,218,119,150,184,123,1,131,12,19,5,181,121,70,61,117,49,48,124,125,207,221)

  fullNridgHtIndices = 
  c(46,73,4,34,68,53,77,16,75,52,41,1,72,44,29,67,8,36,32,6,60,35,19,65,20,62,50,7,38,45,27,33,37,23,11,40,5,76,55,13,43,71,63,48,25,26,18,51,58,49,69,54,22,70,57,59,15,66,74,28,24,21,2,56,64,10,42,61,3,39,9,30,31,12,47,17,14)


  fullMitchelIndices = 
  c(16,20,10,18,7,37,49,39,24,4,48,13,38,27,33,43,1,47,8,11,15,42,3,36,32,45,34,44,12,17,2,29,30,19,40,35,28,5,25,46,21,6,31,26,9,41,22,23,14)

  fullStoneBrIndices = 
  c(25,12,2,15,21,22,6,8,23,4,9,13,14,7,1,18,16,19,11,5,24,20,10,17,3)

#+END_SRC


Now we get our testing and training data from these samples, filtering (using our  and applying the indices we've generated.
#+BEGIN_SRC R :tangle init.R
onlyNAmes = dat[dat$Neighborhood == "NAmes",]
testingNAmes = onlyNAmes[fullNAmesIndices[1:169],]
trainingNAmes = onlyNAmes[fullNAmesIndices[170:225],]

onlyNrigdHt = dat[dat$Neighborhood == "NridgHt",]
testingNridgHt = onlyNrigHt[fullNridgHtIndices[1:58],]
trainingNridgHt = onlyNrigHt[fullNridgHtIndices[59:77],]

onlyMitchel = dat[dat$Neighborhood == "Mitchel",]
testingMitchel = onlyMitchel[fullMitchelIndices[1:35],]
trainingMitchel = onlyMitchel[fullMitchelIndices[36:46],]

onlyStoneBr = dat[dat$Neighborhood == "StoneBr",]
testingStoneBr = onlyStoneBr[fullStoneBrIndices[1:18],]
trainingStoneBr = onlyStoneBr[fullStoneBrIndices[19:25],]
#+END_SRC


Phew. Now that all that work is done, it's easy to just source all of these samples:
#+BEGIN_SRC R
source("init.R")
#+END_SRC

** OpenPorchSF
We've noted that mean shifts quite drastically when we test for inclusion of a porch. The aim of this section is to test for correlation between this and another feature that had a significant effect on average price, WoodDeckSF.

(Initially I looked at just the presence. Here I am comparing numerical values)

First the plot:

#+BEGIN_SRC R
plot(testingNAmes$OpenPorchSF, testingNAmes$WoodDeckSF)
#+END_SRC

[[./img/trainingNAmesPorchDeck.png]]

Nothing is immediately apparent. Let's fall back to tests.

We'll just be using spearman's Rho, and the testing data.
#+BEGIN_SRC R
cor.test(trainingNAmes$OpenPorchSF, trainingNAmes$WoodDeckSF, method="kendall")

# 	Kendall's rank correlation tau

# data:  trainingNAmes$OpenPorchSF and trainingNAmes$WoodDeckSF
# z = 0.72579, p-value = 0.468
# alternative hypothesis: true tau is not equal to 0
# sample estimates:
#        tau 
# 0.08531233 

#+END_SRC

Hmm no correlation. Let's try filtering out all the houses that don't have porches.

#+BEGIN_SRC R
cor.test(trainingNAmes[trainingNAmes$OpenPorchSF != 0,]$OpenPorchSF, trainingNAmes[trainingNAmes$OpenPorchSF != 0,]$WoodDeckSF, method="spearman")


# 	Spearman's rank correlation rho

# data:  trainingNAmes[trainingNAmes$OpenPorchSF != 0, ]$OpenPorchSF and trainingNAmes[trainingNAmes$OpenPorchSF != 0, ]$WoodDeckSF
# S = 1485.8, p-value = 0.08969
# alternative hypothesis: true rho is not equal to 0
# sample estimates:
#       rho 
# 0.3539844 

# Warning message:
# In cor.test.default(trainingNAmes[trainingNAmes$OpenPorchSF != 0,  :
#   Cannot compute exact p-value with ties

#+END_SRC

I'm not sure what the warning message means. But there are only 8 (!) elements in these datasets, which probably means a tie, and also probably means that our close to 0.05 p-value is not reliable.

It seems comparing within the neighborhood is too restrictive. Let's try throughout Boston:

#+BEGIN_SRC R
plot(trainingdat$OpenPorchSF, trainingdat$WoodDeckSF)
#+END_SRC

[[./img/trainingdatPorchDeck.png]]

#+BEGIN_SRC R
cor.test(trainingdat$OpenPorchSF, trainingdat$WoodDeckSF, method="spearman")

# 	Spearman's rank correlation rho

# data:  trainingdat$OpenPorchSF and trainingdat$WoodDeckSF
# S = 199780000, p-value = 0.003951
# alternative hypothesis: true rho is not equal to 0
# sample estimates:
#        rho 
# 0.08703042 

# Warning message:
# In cor.test.default(trainingdat$OpenPorchSF, trainingdat$WoodDeckSF,  :
#   Cannot compute exact p-value with ties
#+END_SRC

Ooh, exciting! However that pesky error message is still there. We can try using kendall's test (which is another rank correlation, and thus does not depend on normality and is robust to outliers:

#+BEGIN_SRC R
> cor.test(trainingdat$OpenPorchSF, trainingdat$WoodDeckSF, method="kendall")

# 	Kendall's rank correlation tau

# data:  trainingdat$OpenPorchSF and trainingdat$WoodDeckSF
# z = 2.958, p-value = 0.003097
# alternative hypothesis: true tau is not equal to 0
# sample estimates:
#        tau 
# 0.06960081 
#+END_SRC

Since the p-value is <0.05 and our results are yes/no, we can conclude that there indeed is a correlation between the sizes of porches and wood decks. A possible next step is finding a glm model to find out the extent of this correlation (is tau a good indicator?). The reason I think this is important is that these two were the biggest contributors to average price increases, so the fact that they are comorbid implies that their effect is diminished.


A possible next step is to perform a complete analysis on the correlations between each aspect of the house. This will be a little time consuming, as there are 6! of these, 720 tests (it can be automated, of course, we have computers).
